{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.feat import ConvMolFeaturizer\n",
    "from deepchem.utils import download_url\n",
    "from deepchem.data import DiskDataset\n",
    "import deepchem as dc\n",
    "from Bio.PDB import PDBParser, PDBIO\n",
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"Full Dataset/\"\n",
    "pdb_files = os.listdir(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for inhibitors\n",
    "def check_for_inhibitors(pdb_file):\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure('structure', pdb_file)\n",
    "\n",
    "    inhibitors = []\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue in chain:\n",
    "                # Checking heteroatoms for inhibitors\n",
    "                if residue.id[0] != ' ':\n",
    "                    inhibitors.append(residue.resname)\n",
    "    \n",
    "    return 1 if inhibitors else 0\n",
    "\n",
    "# check_for_inhibitors(dataset_path + pdb_files[2])\n",
    "\n",
    "featurizer = ConvMolFeaturizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = []\n",
    "# labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pdb_path in pdb_files:\n",
    "    mol = Chem.MolFromPDBFile(dataset_path + pdb_path)\n",
    "    if mol is not None:\n",
    "        try:\n",
    "            featurized_mol = featurizer.featurize([mol])[0]\n",
    "            features.append(featurized_mol)\n",
    "            print(f\"Featurized {pdb_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to featurize {pdb_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        labels.append(check_for_inhibitors(dataset_path + pdb_path))\n",
    "\n",
    "# Convert labels to numpy array\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Ensure features is a list of arrays, not a single numpy array\n",
    "features = np.array(features, dtype=object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(features))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset and save it\n",
    "dataset = DiskDataset.from_numpy(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepchem_dataset = dc.data.DiskDataset.from_numpy(features, labels)\n",
    "deepchem_dataset.move('pdb_full_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.data import Dataset\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "dataset = dc.data.DiskDataset('pdb_full_dataset')\n",
    "\n",
    "\n",
    "# Use a splitter to split the dataset\n",
    "splitter = dc.splits.RandomSplitter()\n",
    "train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(dataset, frac_train=0.8, frac_valid=0.1, frac_test=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.models import GraphConvModel\n",
    "\n",
    "# Define the model\n",
    "model = GraphConvModel(n_tasks=1, mode='classification', n_classes=2, batch_size=50, learning_rate=0.001, model_dir=\"Graph Models/\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, nb_epoch=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set - OLD MODEL\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean)\n",
    "train_score = model.evaluate(train_dataset, [metric])\n",
    "valid_score = model.evaluate(valid_dataset, [metric])\n",
    "test_score = model.evaluate(test_dataset, [metric])\n",
    "\n",
    "print(\"Train AUC: \", train_score)\n",
    "print(\"Validation AUC: \", valid_score)\n",
    "print(\"Test AUC: \", test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set - NEW MODEL\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean)\n",
    "train_score = model.evaluate(train_dataset, [metric])\n",
    "valid_score = model.evaluate(valid_dataset, [metric])\n",
    "test_score = model.evaluate(test_dataset, [metric])\n",
    "\n",
    "print(\"Train AUC: \", train_score)\n",
    "print(\"Validation AUC: \", valid_score)\n",
    "print(\"Test AUC: \", test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set - NEW MODEL trained 5 more epochs\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean)\n",
    "train_score = model.evaluate(train_dataset, [metric])\n",
    "valid_score = model.evaluate(valid_dataset, [metric])\n",
    "test_score = model.evaluate(test_dataset, [metric])\n",
    "\n",
    "print(\"Train AUC: \", train_score)\n",
    "print(\"Validation AUC: \", valid_score)\n",
    "print(\"Test AUC: \", test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_test_path = \"Plasmodium/Mixed Final Dataset Test\"\n",
    "old_test_files = os.listdir(old_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_pdb(model):\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    test_path = os.listdir(\"Plasmodium/Mixed Final Dataset Test\")\n",
    "\n",
    "    for pdb_file in test_path:\n",
    "        print(pdb_file)\n",
    "\n",
    "        pdb_file = \"Plasmodium/Mixed Final Dataset Test/\" + pdb_file\n",
    "\n",
    "        try:\n",
    "\n",
    "            \n",
    "            featurized_mol = featurizer.featurize([mol])[0]\n",
    "\n",
    "\n",
    "            features = pad_features(features, 100 * 24)\n",
    "\n",
    "            features = features.reshape(1, -1)\n",
    "\n",
    "            deepchem_dataset_entry = dc.data.NumpyDataset(features)\n",
    "\n",
    "            predictions = model.predict(deepchem_dataset_entry)\n",
    "\n",
    "            # print(predictions)\n",
    "\n",
    "            probabilities = torch.sigmoid(torch.tensor(predictions)).numpy()\n",
    "            # print(probabilities)\n",
    "            label = (probabilities > 0.5).astype(int)\n",
    "\n",
    "            inhibitors_check = 1 if check_for_inhibitors(pdb_file) else 0\n",
    "\n",
    "            total += 1\n",
    "            \n",
    "            print(f'{label=}, {inhibitors_check=}')\n",
    "\n",
    "            if label == 1:\n",
    "                model_inhibitor_check = 1\n",
    "                print(f\"{pdb_file[0]} contains inhibitors\")\n",
    "            else:\n",
    "                model_inhibitor_check = 0\n",
    "                print(f\"{pdb_file[0]} does not contain inhibitors\")\n",
    "\n",
    "            if inhibitors_check == model_inhibitor_check:\n",
    "                print(\"CORRECT\")\n",
    "                correct += 1\n",
    "            else:\n",
    "                print(\"WRONG\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdb_file}: {e}\")\n",
    "\n",
    "    print(f\"Accuracy: {correct / total * 100:.2f}%\")\n",
    "\n",
    "# Assuming your model is already trained and available as `model`\n",
    "print(\"MODEL PREDICTIONS\")\n",
    "predict_from_pdb(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for pdb_path in pdb_files:\n",
    "#     mol = Chem.MolFromPDBFile(dataset_path + pdb_path)\n",
    "#     if mol is not None:\n",
    "#         try:\n",
    "#             featurized_mol = featurizer.featurize([mol])[0]\n",
    "#             features.append(featurized_mol)\n",
    "#             print(f\"Featurized {pdb_path}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to featurize {pdb_path}: {e}\")\n",
    "#             continue\n",
    "        \n",
    "#         labels.append(check_for_inhibitors(dataset_path + pdb_path))\n",
    "\n",
    "# # Convert labels to numpy array\n",
    "# labels = np.array(labels)\n",
    "\n",
    "# # Ensure features is a list of arrays, not a single numpy array\n",
    "# features = np.array(features, dtype=object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_test_path = \"Plasmodium/Mixed Final Dataset Test\"\n",
    "old_test_files = os.listdir(old_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "wrong = 0\n",
    "\n",
    "for pdb in old_test_files:\n",
    "    # print(old_test_path + \"/\" + pdb)\n",
    "\n",
    "    try:\n",
    "    \n",
    "        features = np.array(featurizer.featurize([Chem.MolFromPDBFile(old_test_path + \"/\" + pdb)]), dtype=object)\n",
    "        prediction = model.predict_on_batch(features)\n",
    "\n",
    "        prediction_label = np.argmax(prediction)\n",
    "        ground_truth = check_for_inhibitors(old_test_path + \"/\" + pdb)\n",
    "\n",
    "        if prediction_label == ground_truth:\n",
    "            print(f\"Correct prediction for {pdb} || Prediction: {prediction_label} || Ground Truth: {ground_truth}\")\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(f\"Incorrect prediction for {pdb} || Prediction: {prediction_label} || Ground Truth: {ground_truth}\")\n",
    "            wrong += 1\n",
    "\n",
    "        total += 1\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdb}: {e}\")\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {correct / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(featurizer.featurize([Chem.MolFromPDBFile(old_test_path + \"/\" + pdb)]), dtype=object)\n",
    "prediction = model.predict_on_batch(features)\n",
    "\n",
    "print(np.argmax(prediction))\n",
    "check_for_inhibitors(old_test_path + \"/\" + pdb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
